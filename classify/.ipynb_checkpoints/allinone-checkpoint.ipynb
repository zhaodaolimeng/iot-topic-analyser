{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import pywt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import mysql.connector as c\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import variation, linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "START_TIMESTAMP = dt.datetime.strptime(\"2016-12-04 00:00:00\", TIME_FORMAT).timestamp()\n",
    "PICKLE_LOADED_FROM_DB = 'p_raw_series_dict.pickle'\n",
    "PICKLE_FEATURES = 'p_label_feature.pickle'\n",
    "\n",
    "conn = c.connect(user='root', password='ictwsn', host='10.22.0.77', database='curiosity_20161204')\n",
    "label_df = pd.read_sql(\"select * from manual_label_t where label!=''\", conn)\n",
    "label_dict = {(val['feed_id'], val['stream_id']): val['label'] for _, val in label_df.iterrows()}\n",
    "\n",
    "if os.path.isfile(PICKLE_LOADED_FROM_DB):\n",
    "    series_dict = pickle.load(open(PICKLE_LOADED_FROM_DB, 'rb'))\n",
    "else:\n",
    "    series_dict = dict()  # 每个(feedid, datastreamid)对应一个(time_at, val)的list\n",
    "    cursor = conn.cursor()\n",
    "    for f_id, s_id in label_dict:\n",
    "        cursor.execute(\"\"\"\n",
    "            select feedid, datastreamid, time_at, val from datapoint_t\n",
    "            where feedid=%s and datastreamid=%s\n",
    "        \"\"\", (f_id, s_id))\n",
    "        series_dict[(f_id, s_id)] = [(time_at, val) for _, _, time_at, val in cursor.fetchall()]\n",
    "        \n",
    "    pickle.dump(series_dict, open(PICKLE_LOADED_FROM_DB, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 计算特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Error detected!\n",
      "Length of label_dict = 1658\n",
      "Length of feature_dict = 1658\n"
     ]
    }
   ],
   "source": [
    "len_of_2days = 1440 * 2\n",
    "f_dict = dict()\n",
    "black_list = []\n",
    "\n",
    "for fd_tuple, series in series_dict.items():\n",
    "    feature = []\n",
    "    samples = []\n",
    "\n",
    "    # 对数据进行插值\n",
    "    tlist = [(series[i][0].timestamp() - START_TIMESTAMP)/60 for i in range(len(series))]\n",
    "    vlist = [series[i][1] for i in range(len(series))]\n",
    "    tlist = [0] + tlist + [len_of_2days]\n",
    "    vlist = [vlist[0]] + vlist + [vlist[-1]]\n",
    "\n",
    "    # print(fd_tuple)\n",
    "    try:\n",
    "        interp_f = interp1d(tlist, vlist)\n",
    "        interp_f = interp1d(tlist, vlist)\n",
    "        for cur_time in range(0, len_of_2days, 10):\n",
    "            v = interp_f(cur_time)  # 每十分钟进行一次采样\n",
    "            samples.append(v/abs(v)*(200 + math.log(abs(v - 200 + 1))) if abs(v) > 200 else v)\n",
    "    except ValueError:\n",
    "        print('Error detected!')\n",
    "        black_list.append(fd_tuple)\n",
    "        continue\n",
    "\n",
    "    # 常规参数\n",
    "    sample_np = np.array(samples)\n",
    "    s_ave = np.average(sample_np)\n",
    "    feature.append(s_ave)\n",
    "    feature.append(variation(sample_np))\n",
    "    feature.append(np.min(sample_np))\n",
    "    feature.append(np.max(sample_np))\n",
    "\n",
    "    # 小波系数\n",
    "    w_coeff = pywt.wavedec(samples, 'haar', level=5)\n",
    "    feature.append(np.linalg.norm(w_coeff[0]))\n",
    "    feature.append(np.linalg.norm(w_coeff[1]))\n",
    "    feature.append(np.linalg.norm(w_coeff[2]))\n",
    "    feature.append(np.linalg.norm(w_coeff[3]))\n",
    "    feature.append(np.linalg.norm(w_coeff[4]))\n",
    "\n",
    "    # 对于均值的zero cross\n",
    "    zc = [i for i in range(1, sample_np.size-1) if (sample_np[i] - s_ave)*(sample_np[i-1] - s_ave) > 0]\n",
    "    feature.append(len(zc))\n",
    "\n",
    "    # 一阶回归之后的整体趋势\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(sample_np.tolist(), list(range(len(sample_np))))\n",
    "    feature.append(slope)\n",
    "    feature.append(intercept)\n",
    "    feature.append(r_value)\n",
    "    feature.append(p_value)\n",
    "    feature.append(std_err)\n",
    "\n",
    "    # 检查数据有效性\n",
    "    is_invalid = False\n",
    "    for f in feature:\n",
    "        if math.isnan(f) or math.isinf(f):\n",
    "            is_invalid = True\n",
    "            break\n",
    "    if not is_invalid:\n",
    "        f_dict[fd_tuple] = feature\n",
    "\n",
    "l_swap_dict = dict()\n",
    "for fs_tuple, f_list in f_dict.items():\n",
    "    l_swap_dict[fs_tuple] = label_dict[fs_tuple]\n",
    "\n",
    "result = {'label_dict': l_swap_dict, 'feature_dict': f_dict}\n",
    "pickle.dump(result, open(PICKLE_FEATURES, 'wb'))\n",
    "print('Length of label_dict = ' + str(len(l_swap_dict)))\n",
    "print('Length of feature_dict = ' + str(len(f_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 分类方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 顺序打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "raw_dict = pickle.load(open('p_label_feature.pickle', 'rb'))\n",
    "label_dict = raw_dict['label_dict']\n",
    "feature_dict = raw_dict['feature_dict']\n",
    "n_fold = 10\n",
    "\n",
    "n_per_fold = int(len(label_dict)/n_fold)\n",
    "all_tuple = [fs_tuple for fs_tuple, v in label_dict.items()]\n",
    "all_label = [label_dict[fs_tuple] for fs_tuple, v in label_dict.items()]\n",
    "all_feature = [feature_dict[fs_tuple] for fs_tuple in all_tuple]\n",
    "\n",
    "bundle = [(all_tuple[i], all_label[i], all_feature[i]) for i,_ in enumerate(all_tuple)]\n",
    "shuffle(bundle)\n",
    "for i, _ in enumerate(bundle):\n",
    "    all_tuple[i] = bundle[i][0]\n",
    "    all_label[i] = bundle[i][1]\n",
    "    all_feature[i] = bundle[i][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc = 54.54545454545455\n",
      "Acc = 63.63636363636363\n",
      "Acc = 63.03030303030303\n",
      "Acc = 58.78787878787879\n",
      "Acc = 64.24242424242425\n",
      "Acc = 66.66666666666667\n",
      "Acc = 67.87878787878788\n",
      "Acc = 62.42424242424242\n",
      "Acc = 66.06060606060606\n",
      "Acc = 62.42424242424242\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_fold):\n",
    "    \n",
    "    test_tuple = all_tuple[i*n_per_fold:(i+1)*n_per_fold]\n",
    "    test_label = all_label[i*n_per_fold:(i+1)*n_per_fold]\n",
    "    test_feature = all_feature[i*n_per_fold:(i+1)*n_per_fold]\n",
    "    \n",
    "    train_tuple = all_tuple[0:i*n_per_fold] + all_tuple[(i+1)*n_per_fold+1:-1]\n",
    "    train_label = all_label[0:i*n_per_fold] + all_label[(i+1)*n_per_fold+1:-1]\n",
    "    train_feature = all_feature[0:i*n_per_fold] + all_feature[(i+1)*n_per_fold+1:-1]\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    rfc.fit(train_feature, train_label)\n",
    "    preds = rfc.predict(test_feature)\n",
    "\n",
    "    acc_list = [p for i,p in enumerate(preds) if p==test_label[i]]\n",
    "    print('Acc = ' + str(100.0*len(acc_list)/len(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_names = []\n",
    "for L in train_label + test_label:\n",
    "    if L not in label_names:\n",
    "        label_names.append(L)\n",
    "\n",
    "cm = confusion_matrix(test_label, preds)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm.tolist())\n",
    "fig.colorbar(cax)\n",
    "# ax.set_xticklabels([''] + label_names, rotation=90)\n",
    "ax.set_yticklabels([''] + label_names)\n",
    "# ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Ground Truth Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 分类优化\n",
    "\n",
    "Random forest可以得到不同分类的概率，通过结合组别信息，可以得到更为精确的分类结果。\n",
    "一个应用包含多种不同的传感器，故可以使用其中包含的传感器配比信息区别不同应用。\n",
    "所以设计了以下算法进行优化：\n",
    "1. 使用k-means对不同应用进行聚类，表现在数据集中则是对feedid进行归类\n",
    "2. 对于同一组应用中的不同传感器，可以计算得到每个传感器属于不同类型的综合打分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def kmeans(features, n_mu=10, max_iter=10):\n",
    "    cluster_label = [0] * len(features)\n",
    "    mu_list = np.random.rand(n_mu, len(features[0]))  # 每个聚类的中心K*27\n",
    "    mu_list /= np.matrix(np.sum(mu_list, axis=1)).T\n",
    "\n",
    "    for iter_times in range(max_iter):\n",
    "        for i, x in enumerate(features):\n",
    "            distance = np.asarray([sum((mu - x) ** 2) for mu in mu_list])\n",
    "            cluster_label[i] = distance.argmin()\n",
    "\n",
    "        mu_count = [0] * n_mu\n",
    "        mu_sum = np.zeros((n_mu, len(features[0])))\n",
    "\n",
    "        for i, x in enumerate(features):\n",
    "            mu_sum[cluster_label[i]] += x\n",
    "            mu_count[cluster_label[i]] += 1\n",
    "\n",
    "        for idx, mu_feature in enumerate(mu_list):\n",
    "            if mu_count[idx] != 0:\n",
    "                mu_list[idx] = mu_sum[idx] / mu_count[idx]\n",
    "    return cluster_label, mu_list\n",
    "\n",
    "\n",
    "def adjust_sensor_choice(sensor_proba, type_target, portion_lambda=0.01, rounding_scale=10):\n",
    "    n_sensors = len(sensor_proba)\n",
    "    graph_edge = []\n",
    "    for idx, p_si in enumerate(sensor_proba):\n",
    "        graph_edge.append((0, 1 + idx, {'capacity': 1, 'weight': 0}))\n",
    "        for idx_t, _ in enumerate(type_target):\n",
    "            if p_si[idx_t] != 0:\n",
    "                st = {'capacity': 1, 'weight': 1.0 / p_si[idx_t]}\n",
    "                graph_edge.append((1 + idx, 1 + idx_t + n_sensors, st))\n",
    "\n",
    "    for idx_t, t in enumerate(type_target):\n",
    "        s = sum(type_target)\n",
    "        if t != 0:\n",
    "            st = {'capacity': n_sensors, 'weight': 1.0 * portion_lambda * s / t}\n",
    "            graph_edge.append((1 + n_sensors + idx_t, 1 + n_sensors + len(type_target), st))\n",
    "\n",
    "    # 当weight和capacity为浮点数时方法失效，所以将权重进行rounding\n",
    "    min_weight = min([e[2]['weight'] for e in graph_edge if e[2]['weight'] > 0])\n",
    "    ratio = rounding_scale / min_weight\n",
    "    for e in graph_edge:\n",
    "        e[2]['weight'] = int(ratio * e[2]['weight'])\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_edges_from(graph_edge)\n",
    "    flow_dict = nx.max_flow_min_cost(graph, 0, 1 + n_sensors + len(type_target))\n",
    "\n",
    "    choice = []\n",
    "    for node_idx in range(n_sensors):\n",
    "        if 1 + node_idx in flow_dict:\n",
    "            alist = [to for to, c in flow_dict[1 + node_idx].items() if c == 1]\n",
    "            choice.append(np.argmax(sensor_proba[node_idx]) if len(alist) == 0 else alist[0] - n_sensors - 1)\n",
    "    return choice\n",
    "\n",
    "\n",
    "def result_refine(tuples, sensor_p, choices, max_iter=3):\n",
    "    \"\"\"\n",
    "    输入的tuples和其他对应的位全部是乱序的\n",
    "    返回sensor类型的选择状态sensor_choice\n",
    "    \"\"\"\n",
    "    feedid = [tuples[i][0] for i, _ in enumerate(tuples)]\n",
    "    n_types = len(sensor_p[0])\n",
    "\n",
    "    group_p = []  # 存储每个组的概率\n",
    "    group_id = []  # 存储每个组的id\n",
    "    group_cnt_dict = dict()\n",
    "    for i in range(len(tuples)):\n",
    "        if tuples[i][0] not in group_cnt_dict:\n",
    "            group_cnt_dict[tuples[i][0]] = [0]*n_types\n",
    "        cnt = group_cnt_dict[tuples[i][0]]\n",
    "        cnt[choices[i]] += 1\n",
    "\n",
    "    for k, v in group_cnt_dict.items():\n",
    "        group_id.append(k)\n",
    "        group_p.append(np.asarray(v) / np.sum(v))\n",
    "\n",
    "    print('group_id, group_p prepared!')\n",
    "\n",
    "    for run_once in range(max_iter):\n",
    "        cluster_label, mu_list = kmeans(group_p, max_iter=10)  # 执行一次聚类\n",
    "        print('k-means done!')\n",
    "\n",
    "        group_dict = dict()  # 找到每个组对应的sensor的id\n",
    "        for idx, f in enumerate(feedid):\n",
    "            if f not in group_dict:\n",
    "                group_dict[f] = []\n",
    "            group_dict[f].append(idx)\n",
    "\n",
    "        for idx, cluster in enumerate(cluster_label):\n",
    "            sensor_proba = []  #当前组每个节点的分布 n*len(features[0])\n",
    "            sensor_id_list = []\n",
    "            for sensor_idx in group_dict[group_id[idx]]:\n",
    "                sensor_proba.append(sensor_p[sensor_idx])\n",
    "                sensor_id_list.append(sensor_idx)\n",
    "\n",
    "            choice = adjust_sensor_choice(sensor_proba, mu_list[cluster])\n",
    "            for inner_idx, sensor_idx in enumerate(sensor_id_list):\n",
    "                choices[sensor_idx] = choice[inner_idx]\n",
    "\n",
    "        print('Adjust done!')\n",
    "\n",
    "    return choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用单次的数据进行测试：\n",
    "1. 使用“十一”划分，并使用random forest计算出前1/10的每个数据条目的概率\n",
    "2. 将概率和训练集中的进行拼接，all_proba，即每个条目对应的概率\n",
    "3. 调用result_refine方法重新计算每个条目的类型sensor_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行整体的交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9680337756332931\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.9686369119420989\n"
     ]
    }
   ],
   "source": [
    "def compute_acc(y, y_test):\n",
    "    acc = len([c_y for idx_y, c_y in enumerate(y) if c_y == y_test[idx_y]])\n",
    "    return 1.0 * acc / len(y)\n",
    "\n",
    "\n",
    "raw_dict = pickle.load(open('p_label_feature.pickle', 'rb'))\n",
    "label_dict = raw_dict['label_dict']\n",
    "feature_dict = raw_dict['feature_dict']\n",
    "\n",
    "n_per_fold = int(len(label_dict)/n_fold)\n",
    "all_tuple = [fs_tuple for fs_tuple, v in label_dict.items()]\n",
    "all_label = [label_dict[fs_tuple] for fs_tuple in all_tuple]\n",
    "all_feature = [feature_dict[fs_tuple] for fs_tuple in all_tuple]\n",
    "\n",
    "bundle = [(all_tuple[i], all_label[i], all_feature[i]) for i in range(len(all_tuple))]\n",
    "shuffle(bundle)\n",
    "for i in range(len(bundle)):\n",
    "    all_tuple[i] = bundle[i][0]\n",
    "    all_label[i] = bundle[i][1]\n",
    "    all_feature[i] = bundle[i][2]\n",
    "\n",
    "test_tuple = all_tuple[0:n_per_fold]\n",
    "test_label = all_label[0:n_per_fold]\n",
    "test_feature = all_feature[0:n_per_fold]\n",
    "train_tuple = all_tuple[n_per_fold:]\n",
    "train_label = all_label[n_per_fold:]\n",
    "train_feature = all_feature[n_per_fold:]\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(train_feature, train_label)\n",
    "\n",
    "index2name = rfc.classes_  # 概率对应位置的标签\n",
    "name2index = {cls: idx for idx, cls in enumerate(rfc.classes_)}\n",
    "test_proba = rfc.predict_proba(test_feature).tolist()\n",
    "\n",
    "for i in range(len(train_tuple)):\n",
    "    arr = [0.0] * len(index2name)\n",
    "    arr[name2index[train_label[i]]] = 1.0\n",
    "    test_proba.append(arr)\n",
    "\n",
    "sensor_choice = [np.argmax(p) for p in test_proba]\n",
    "y_before = [index2name[idx] for idx in sensor_choice]\n",
    "print(compute_acc(all_label, y_before))\n",
    "sensor_choice = result_refine(all_tuple, test_proba, sensor_choice)  # 结果优化\n",
    "y_after = [index2name[idx] for idx in sensor_choice]\n",
    "print(compute_acc(all_label, y_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6303030303030303\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6424242424242425\n",
      "0.6\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6242424242424243\n",
      "0.6181818181818182\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6363636363636364\n",
      "0.5333333333333333\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.5636363636363636\n",
      "0.6424242424242425\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6666666666666666\n",
      "0.6545454545454545\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6727272727272727\n",
      "0.6242424242424243\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6484848484848484\n",
      "0.6303030303030303\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6424242424242425\n",
      "0.6242424242424243\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6484848484848484\n",
      "0.6545454545454545\n",
      "group_id, group_p prepared!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "k-means done!\n",
      "Adjust done!\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_fold):\n",
    "    test_tuple = all_tuple[i * n_per_fold:(i + 1) * n_per_fold]\n",
    "    test_label = all_label[i * n_per_fold:(i + 1) * n_per_fold]\n",
    "    test_feature = all_feature[i * n_per_fold:(i + 1) * n_per_fold]\n",
    "    train_tuple = all_tuple[0:i * n_per_fold] + all_tuple[(i + 1) * n_per_fold:]\n",
    "    train_label = all_label[0:i * n_per_fold] + all_label[(i + 1) * n_per_fold:]\n",
    "    train_feature = all_feature[0:i * n_per_fold] + all_feature[(i + 1) * n_per_fold:]\n",
    "\n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    rfc.fit(train_feature, train_label)\n",
    "\n",
    "    index2name = rfc.classes_  # 概率对应位置的标签\n",
    "    name2index = {cls: idx for idx, cls in enumerate(rfc.classes_)}\n",
    "    result_proba = rfc.predict_proba(test_feature).tolist()\n",
    "\n",
    "    # 1. 使用常规方法\n",
    "    sensor_choice = [np.argmax(p) for p in result_proba]\n",
    "    y_before = [index2name[idx] for idx in sensor_choice]\n",
    "    print(compute_acc(test_label, y_before))\n",
    "\n",
    "    # 2. 使用常规方法+优化\n",
    "    test_proba = []\n",
    "    for ii in range(i*n_per_fold):\n",
    "        arr = [0.0] * len(index2name)\n",
    "        arr[name2index[train_label[ii]]] = 1.0\n",
    "        test_proba.append(arr)\n",
    "    test_proba += result_proba\n",
    "    for ii in range(i*n_per_fold, len(train_tuple)):\n",
    "        arr = [0.0] * len(index2name)\n",
    "        arr[name2index[train_label[ii]]] = 1.0\n",
    "        test_proba.append(arr)\n",
    "    sensor_choice = [np.argmax(p) for p in test_proba]\n",
    "    sensor_choice = result_refine(all_tuple, test_proba, sensor_choice)  # 结果优化\n",
    "    y_after = [index2name[idx] for idx in sensor_choice[i*n_per_fold:(i+1)*n_per_fold]]\n",
    "    print(compute_acc(test_label, y_after))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
