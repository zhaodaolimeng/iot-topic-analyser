{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import pywt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import mysql.connector as c\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import variation, linregress\n",
    "\n",
    "feed_dict = pickle.load(open('feed.pickle','rb'))\n",
    "time_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "epoch_time = dt.datetime.strptime('2017-04-01T00:00:00Z', time_format).timestamp()\n",
    "\n",
    "def magic(x, threshold=200):\n",
    "    return x/abs(x)*(threshold + math.log(abs(x - threshold + 1))) if abs(x) > threshold else x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feature_single(series):\n",
    "    \n",
    "    len_of_2days = 1440 * 2\n",
    "    f_dict = dict()\n",
    "    feature = []\n",
    "    samples = []\n",
    "    \n",
    "    # 对数据进行插值\n",
    "    tlist = [(series[i][0].timestamp() - epoch_time)/60 for i in range(len(series))]\n",
    "    vlist = [series[i][1] for i in range(len(series))]\n",
    "    tlist = [0.0] + tlist + [len_of_2days]\n",
    "    vlist = [vlist[0]] + vlist + [vlist[-1]]\n",
    "    try:\n",
    "        interp_f = interp1d(tlist, vlist)\n",
    "        for cur_time in range(0, len_of_2days, 10):\n",
    "            v = interp_f(cur_time)  # 每十分钟进行一次采样\n",
    "            samples.append(magic(v))\n",
    "    except ValueError:\n",
    "        print('Detect error!')\n",
    "        return None\n",
    "\n",
    "    # 常规参数\n",
    "    sample_np = np.array(samples)\n",
    "    s_ave = np.average(sample_np)\n",
    "    feature.append(s_ave)\n",
    "    feature.append(variation(sample_np))\n",
    "    feature.append(np.min(sample_np))\n",
    "    feature.append(np.max(sample_np))\n",
    "\n",
    "    # 小波系数\n",
    "    w_coeff = pywt.wavedec(samples, 'haar', level=5)\n",
    "    feature.append(np.linalg.norm(w_coeff[0]))\n",
    "    feature.append(np.linalg.norm(w_coeff[1]))\n",
    "    feature.append(np.linalg.norm(w_coeff[2]))\n",
    "    feature.append(np.linalg.norm(w_coeff[3]))\n",
    "    feature.append(np.linalg.norm(w_coeff[4]))\n",
    "\n",
    "    # 对于均值的zero cross\n",
    "    zc = [i for i in range(1, sample_np.size-1) if (sample_np[i] - s_ave)*(sample_np[i-1] - s_ave) > 0]\n",
    "    feature.append(len(zc))\n",
    "\n",
    "    # 一阶回归之后的整体趋势\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(sample_np.tolist(), list(range(len(sample_np))))\n",
    "    feature.append(slope)\n",
    "    feature.append(intercept)\n",
    "    feature.append(r_value)\n",
    "    feature.append(p_value)\n",
    "    feature.append(std_err)\n",
    "\n",
    "    # 以最大值作为分裂两部分数据的依据\n",
    "    # 分别计算两部分数据的最小值，作为单日数据截断的依据\n",
    "    # maxv_idx = np.argmax(sample_np)\n",
    "    # minv_idx = np.argmax(-sample_np)\n",
    "    # feature.append()\n",
    "\n",
    "    # 检查数据有效性\n",
    "    for f in feature:\n",
    "        if math.isnan(f) or math.isinf(f):\n",
    "            return None # invalid\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# 抽取每个设备的序列数据\n",
    "feature_dict = {}\n",
    "\n",
    "for counter, (feedid, value) in enumerate(feed_dict.items()):\n",
    "    content, field_name_mapping = value\n",
    "    series_dict = {}\n",
    "    \n",
    "    for entry in content:\n",
    "        time_stamp = dt.datetime.strptime(entry['created_at'], time_format)\n",
    "        for k,v in entry.items():\n",
    "            re_result = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", str(v)) if not v is None else None\n",
    "            numerical_v = float(re_result.group()) if not re_result is None else 0.0\n",
    "            if k.startswith('field'):\n",
    "                if (feedid,k) not in series_dict:\n",
    "                    series_dict[(feedid, k)] = []\n",
    "                series_dict[(feedid, k)].append((time_stamp, numerical_v))\n",
    "    \n",
    "    for (feedid, fieldid), vlist in series_dict.items():\n",
    "        feature_result = feature_single(vlist)\n",
    "        if feature_result is None:\n",
    "            continue\n",
    "        feature_dict[(feedid, fieldid)] = feature_result\n",
    "    \n",
    "    if counter % 100 == 0:\n",
    "        print(counter)\n",
    "\n",
    "pickle.dump(feature_dict, open('feature.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 方法调用，从数据库中进行读取\n",
    "# \n",
    "np.random.seed(0)\n",
    "\n",
    "TIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "START_TIMESTAMP = dt.datetime.strptime(\"2016-12-04 00:00:00\", TIME_FORMAT).timestamp()\n",
    "PICKLE_LOADED_FROM_DB = 'p_raw_series_dict.pickle'\n",
    "PICKLE_FEATURES = 'p_label_feature.pickle'\n",
    "\n",
    "db_conn = c.connect(user='root', password='ictwsn', host='10.22.0.77', database='curiosity_thingspeak')\n",
    "print('Fetch labeled streams ...')\n",
    "label_df = pd.read_sql(\"select * from manual_label_t where label!=''\", db_conn)\n",
    "label_dict = {(val['feed_id'], val['stream_id']): val['label'] for _, val in label_df.iterrows()}\n",
    "\n",
    "if os.path.isfile(PICKLE_LOADED_FROM_DB):\n",
    "    raw_series_dict = pickle.load(open(PICKLE_LOADED_FROM_DB, 'rb'))\n",
    "else:\n",
    "    # 数据集有变动时要删除同目录下的两个pickle文件\n",
    "    raw_series_dict = fetch_raw_datapoints(db_conn, label_dict)\n",
    "    pickle.dump(raw_series_dict, open(PICKLE_LOADED_FROM_DB, 'wb'))\n",
    "\n",
    "print(\"Compute features for each datastream...\")\n",
    "feature_dict = compute_feature(raw_series_dict)\n",
    "\n",
    "l_swap_dict = dict()\n",
    "for fs_tuple, f_list in feature_dict.items():\n",
    "    l_swap_dict[fs_tuple] = label_dict[fs_tuple]\n",
    "\n",
    "result = {'label_dict': l_swap_dict, 'feature_dict': feature_dict}\n",
    "pickle.dump(result, open(PICKLE_FEATURES, 'wb'))\n",
    "print('Length of label_dict = ' + str(len(l_swap_dict)))\n",
    "print('Length of feature_dict = ' + str(len(feature_dict)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
